reward_model_parameters:
  bert: "distilbert/distilbert-base-cased"
  per_device_train_batch_size: 64
  num_epochs: 1
  gradient_checkpointing: true
  remove_unused_columns: false
  max_length: 512
  
  
warp_model_parameters:
  sft_model: "lvwerra/gpt2-imdb"
  reward_model: "weights/checkpoint-196" 
  iterations: 2
  m: 2
  t: 100
  mu: 0.01
  lambda: 0.5
  nu: 0.5
  batch_size: 64
  learning_rate: 0.0003
  beta: 0.25
  alpha: 0.7
  max_new_tokens: 20
  
general:
  output_dir: "weights/"
  
test_allignment:
  llm: "weights/alligned_llm"
  token: "weights/alligned_llm_token"
  
dataset:
  dataset_name: "stanfordnlp/imdb"
  test_max_len: 200
  train_max_len: 400